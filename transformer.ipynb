{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EEAXXuMVXnT3"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import softmax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try to finish this function on your own\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          query: (batch_size, num_heads, seq_len_q, d_k)\n",
        "          key: (batch_size, num_heads, seq_len_k, d_k)\n",
        "          value: (batch_size, num_heads, seq_len_v, d_v)\n",
        "          mask: Optional mask to prevent attention to certain positions\n",
        "      \"\"\"\n",
        "      #Shape checks\n",
        "      assert query.dim() == 4, f\"Query should have 4 dimensions, got {query.dim()}\"\n",
        "      assert key.dim() == 4, f\"Key should have 4 dimensions, got {key.dim()}\"\n",
        "      assert value.dim() == 4, f\"Value should have 4 dimensions, got {value.dim()}\"\n",
        "      assert key.size(-1) == query.size(-1), f\"Key and query depth must be equal\"\n",
        "      assert key.size(-2) == value.size(-2), f\"Key and value sequence length must be equal\"\n",
        "      # get the size of d_k using the query or the key\n",
        "      d_k = query.size(-1)\n",
        "\n",
        "      # calculate the attention score using the formula given. Be vary of the dimension of Q and K. And what you need to transpose to achieve the desired results.\n",
        "      attention_score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "      # hint 1: batch_size and num_heads should not change\n",
        "      # hint 2: nXm @ mXn -> nXn, but you cannot do nXm @ nXm, the right dimension of the left matrix should match the left dimension of the right matrix. The easy way I visualize it is as, who face each other must be same\n",
        "\n",
        "      # add inf is a mask is given, This is used for the decoder layer. You can use help for this if you want to. I did!!\n",
        "      if mask is not None:\n",
        "        attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "      # get the attention weights by taking a softmax on the scores, again be wary of the dimensions. You do not want to take softmax of batch_size or num_heads. Only of the values. How can you do that?\n",
        "      attention_weight = softmax(attention_score, dim=-1)\n",
        "\n",
        "      # return the attention by multiplying the attention weights with the Value (V)\n",
        "      return torch.matmul(attention_weight, value)\n"
      ],
      "metadata": {
        "id": "jRZM100RYHwS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    #Let me write the initializer just for this class, so you get an idea of how it needs to be done\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" #think why?\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # Note: use integer division //\n",
        "\n",
        "        # Create the learnable projection matrices\n",
        "        self.W_q = nn.Linear(d_model, d_model) #think why we are doing from d_model -> d_model\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    @staticmethod\n",
        "    def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          query: (batch_size, num_heads, seq_len_q, d_k)\n",
        "          key: (batch_size, num_heads, seq_len_k, d_k)\n",
        "          value: (batch_size, num_heads, seq_len_v, d_v)\n",
        "          mask: Optional mask to prevent attention to certain positions\n",
        "      \"\"\"\n",
        "      #Shape checks\n",
        "      assert query.dim() == 4, f\"Query should have 4 dimensions, got {query.dim()}\"\n",
        "      assert key.dim() == 4, f\"Key should have 4 dimensions, got {key.dim()}\"\n",
        "      assert value.dim() == 4, f\"Value should have 4 dimensions, got {value.dim()}\"\n",
        "      assert key.size(-1) == query.size(-1), f\"Key and query depth must be equal\"\n",
        "      assert key.size(-2) == value.size(-2), f\"Key and value sequence length must be equal\"\n",
        "      # get the size of d_k using the query or the key\n",
        "      d_k = query.size(-1)\n",
        "\n",
        "      # calculate the attention score using the formula given. Be vary of the dimension of Q and K. And what you need to transpose to achieve the desired results.\n",
        "      attention_score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "      # hint 1: batch_size and num_heads should not change\n",
        "      # hint 2: nXm @ mXn -> nXn, but you cannot do nXm @ nXm, the right dimension of the left matrix should match the left dimension of the right matrix. The easy way I visualize it is as, who face each other must be same\n",
        "\n",
        "      # add inf is a mask is given, This is used for the decoder layer. You can use help for this if you want to. I did!!\n",
        "      if mask is not None:\n",
        "        attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "      # get the attention weights by taking a softmax on the scores, again be wary of the dimensions. You do not want to take softmax of batch_size or num_heads. Only of the values. How can you do that?\n",
        "      attention_weight = softmax(attention_score, dim=-1)\n",
        "\n",
        "      # return the attention by multiplying the attention weights with the Value (V)\n",
        "      return torch.matmul(attention_weight, value)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "      #get batch_size and sequence length\n",
        "      batch_size = query.size(0)\n",
        "      seq_len = query.size(1)\n",
        "\n",
        "      # 1. Linear projections\n",
        "      Q = self.W_q(query)\n",
        "      K = self.W_k(key)\n",
        "      V = self.W_v(value)\n",
        "\n",
        "      # 2. Split into heads\n",
        "      Q = Q.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
        "      K = K.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
        "      V = V.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
        "\n",
        "      # 3. Apply attention\n",
        "      Z = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "      # 4. Concatenate heads\n",
        "      Z = Z.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "      # 5. Final projection\n",
        "      return self.W_o(Z)"
      ],
      "metadata": {
        "id": "MDOy41Gfqipn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    \"\"\"Position-wise Feed-Forward Network\n",
        "\n",
        "    Args:\n",
        "        d_model: input/output dimension\n",
        "        d_ff: hidden dimension\n",
        "        dropout: dropout rate (default=0.1)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "vvvfI8CjxY-c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of shape (max_seq_length, d_model)\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "        # Create position vector\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Create division term\n",
        "        div_term = torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n",
        "        div_term = torch.exp(div_term)\n",
        "\n",
        "        # Compute positional encodings\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register buffer\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "uOqqi84F0UAT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Multi-head attention\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # 2. Layer normalization\n",
        "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # 3. Feed forward\n",
        "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
        "\n",
        "        # 4. Another layer normalization\n",
        "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # 5. Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "            mask: Optional mask for padding\n",
        "        Returns:\n",
        "            x: Output tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 1. Multi-head attention with residual connection and layer norm\n",
        "        attention_result = self.mha(x, x, x, mask)\n",
        "        x = self.dropout(x + attention_result) #Applying dropout after residual connection\n",
        "        x = self.layer_norm_1(x)\n",
        "\n",
        "        # 2. Feed forward with residual connection and layer norm\n",
        "        ffn_result = self.ffn(x)\n",
        "        x = self.dropout(x + ffn_result) #Applying dropout after residual connection\n",
        "        x = self.layer_norm_2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "4E2Yxiol-Inn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Masked Multi-head attention\n",
        "        self.mha_1 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # 2. Layer norm for first sub-layer\n",
        "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # 3. Multi-head attention for cross attention with encoder output\n",
        "        # This will take encoder output as key and value\n",
        "        self.mha_2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # 4. Layer norm for second sub-layer\n",
        "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # 5. Feed forward network\n",
        "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
        "\n",
        "        # 6. Layer norm for third sub-layer\n",
        "        self.layer_norm_3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # 7. Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Target sequence embedding (batch_size, target_seq_len, d_model)\n",
        "            encoder_output: Output from encoder (batch_size, source_seq_len, d_model)\n",
        "            src_mask: Mask for source padding\n",
        "            tgt_mask: Mask for target padding and future positions\n",
        "        \"\"\"\n",
        "        # 1. Masked self-attention\n",
        "        # Remember: In decoder self-attention, query, key, value are all x\n",
        "        attention_result = self.mha_1(x, x, x, tgt_mask)\n",
        "        x = self.dropout(x + attention_result)\n",
        "        x = self.layer_norm_1(x)\n",
        "\n",
        "        attention_result_2 = self.mha_2(x, encoder_output, encoder_output, src_mask)\n",
        "        x = self.dropout(x + attention_result_2)\n",
        "        x = self.layer_norm_2(x)\n",
        "\n",
        "        ff_output = x.self.ffn(x)\n",
        "        x = self.dropout(x + ff_output)\n",
        "        x = self.layer_norm_3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "DtoqrqkuCfbk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 num_layers=6,\n",
        "                 num_heads=8,\n",
        "                 d_ff=2048,\n",
        "                 dropout=0.1,\n",
        "                 max_seq_length=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Input embedding\n",
        "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
        "        self.scale = math.sqrt(d_model)\n",
        "\n",
        "        # 2. Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # 3. Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 4. Stack of N encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for i in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tokens (batch_size, seq_len)\n",
        "            mask: Mask for padding positions\n",
        "        Returns:\n",
        "            encoder_output: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 1. Pass through embedding layer and scale\n",
        "        x = self.embeddings(x) * self.scale\n",
        "\n",
        "        # 2. Add positional encoding and apply dropout\n",
        "        x = self.dropout(self.positional_encoding(x))\n",
        "\n",
        "        # 3. Pass through each encoder layer\n",
        "        for layer in self.encoder_layers:\n",
        "          x = layer(x, mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "DldoWFORR6Er"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 num_layers=6,\n",
        "                 num_heads=8,\n",
        "                 d_ff=2048,\n",
        "                 dropout=0.1,\n",
        "                 max_seq_length=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Output embedding\n",
        "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
        "        self.scale = math.sqrt(d_model)\n",
        "\n",
        "        # 2. Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # 3. Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 4. Stack of N encoder layers\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for i in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Target tokens (batch_size, target_seq_len)\n",
        "            encoder_output: Output from encoder (batch_size, source_seq_len, d_model)\n",
        "            src_mask: Mask for source padding\n",
        "            tgt_mask: Mask for target padding and future positions\n",
        "        Returns:\n",
        "            decoder_output: (batch_size, target_seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 1. Pass through embedding layer and scale\n",
        "        x = self.embeddings(x) * self.scale\n",
        "\n",
        "        # 2. Add positional encoding and apply dropout\n",
        "        x = self.dropout(self.positional_encoding(x))\n",
        "\n",
        "        # 3. Pass through each encoder layer\n",
        "        for layer in self.decoder_layers:\n",
        "          x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PrlgKaDDX785"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "    \"\"\"\n",
        "    Create mask for padding tokens (0s)\n",
        "    Args:\n",
        "        seq: Input sequence tensor (batch_size, seq_len)\n",
        "    Returns:\n",
        "        mask: Padding mask (batch_size, 1, 1, seq_len)\n",
        "    \"\"\"\n",
        "    batch_size, seq_len = seq.shape\n",
        "    output = torch.eq(seq, 0).float()\n",
        "    return output.view(batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_future_mask(size):\n",
        "    \"\"\"\n",
        "    Create mask to prevent attention to future positions\n",
        "    Args:\n",
        "        size: Size of square mask (target_seq_len)\n",
        "    Returns:\n",
        "        mask: Future mask (1, 1, size, size)\n",
        "    \"\"\"\n",
        "    # Create upper triangular matrix and invert it\n",
        "    mask = torch.triu(torch.ones((1, 1, size, size)), diagonal=1) == 0\n",
        "    return mask\n",
        "\n",
        "def create_masks(src, tgt):\n",
        "    \"\"\"\n",
        "    Create all masks needed for training\n",
        "    Args:\n",
        "        src: Source sequence (batch_size, src_len)\n",
        "        tgt: Target sequence (batch_size, tgt_len)\n",
        "    Returns:\n",
        "        src_mask: Padding mask for encoder\n",
        "        tgt_mask: Combined padding and future mask for decoder\n",
        "    \"\"\"\n",
        "    # 1. Create padding masks\n",
        "    src_padding_mask = create_padding_mask(src)\n",
        "    tgt_padding_mask = create_padding_mask(tgt)\n",
        "\n",
        "    # 2. Create future mask\n",
        "    tgt_len = tgt.size(1)\n",
        "    tgt_future_mask = create_future_mask(tgt_len)\n",
        "\n",
        "    # 3. Combine padding and future mask for target\n",
        "    # Both masks should be True for allowed positions\n",
        "    tgt_mask = tgt_padding_mask & tgt_future_mask\n",
        "\n",
        "    return src_padding_mask, tgt_mask"
      ],
      "metadata": {
        "id": "X4_MeBH9ZdfU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size,\n",
        "                 tgt_vocab_size,\n",
        "                 d_model,\n",
        "                 num_layers=6,\n",
        "                 num_heads=8,\n",
        "                 d_ff=2048,\n",
        "                 dropout=0.1,\n",
        "                 max_seq_length=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        # Pass all necessary parameters to Encoder and Decoder\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            d_model,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            d_ff,\n",
        "            dropout,\n",
        "            max_seq_length\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            tgt_vocab_size,\n",
        "            d_model,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            d_ff,\n",
        "            dropout,\n",
        "            max_seq_length\n",
        "        )\n",
        "\n",
        "        # The final linear layer should project from d_model to tgt_vocab_size\n",
        "        self.final_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks for source and target\n",
        "        src_mask, tgt_mask = create_masks(src, tgt)\n",
        "\n",
        "        # Pass through encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "\n",
        "        # Pass through decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Project to vocabulary size\n",
        "        output = self.final_layer(decoder_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "06Rh7DTWaoJ-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLRScheduler:\n",
        "    def __init__(self, optimizer, d_model, warmup_steps):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            optimizer: Optimizer to adjust learning rate for\n",
        "            d_model: Model dimensionality\n",
        "            warmup_steps: Number of warmup steps\n",
        "        \"\"\"\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "\n",
        "    def step(self, step_num):\n",
        "        \"\"\"\n",
        "        Update learning rate based on step number\n",
        "        \"\"\"\n",
        "        lrate = torch.pow(self.d_model,-0.5)*torch.min(torch.pow(step_num,-0.5), torch.tensor(step_num) * torch.pow(self.warmup_steps,-1.5))\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            logits: Model predictions (batch_size, vocab_size) #each row of vocab_size contains probability score of each label\n",
        "            target: True labels (batch_size) #each row of batch size contains the index to the correct label\n",
        "        \"\"\"\n",
        "        vocab_size = logits.size(-1)\n",
        "        with torch.no_grad():\n",
        "            # Create a soft target distribution\n",
        "            true_dist = torch.zeros_like(logits) #create the zeros [0,0,...]\n",
        "            true_dist.fill_(self.smoothing / (vocab_size - 1)) #fill with calculated value [0.000125..,0.000125...] (this is an arbitarary value for example purposes)\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence) #add 1 to the correct index (read more on docs of pytorch)\n",
        "        return torch.mean(torch.sum(-true_dist * torch.log_softmax(logits, dim=-1), dim=-1)) #return cross entropy loss"
      ],
      "metadata": {
        "id": "AKZAnz71b2-2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(model, train_dataloader, criterion, optimizer, scheduler, num_epochs, device='cuda'):\n",
        "    \"\"\"\n",
        "    Training loop for transformer\n",
        "\n",
        "    Args:\n",
        "        model: Transformer model\n",
        "        train_dataloader: DataLoader for training data\n",
        "        criterion: Loss function (with label smoothing)\n",
        "        optimizer: Optimizer\n",
        "        scheduler: Learning rate scheduler\n",
        "        num_epochs: Number of training epochs\n",
        "    \"\"\"\n",
        "    # 1. Setup\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # For tracking training progress\n",
        "    total_loss = 0\n",
        "    all_losses = []\n",
        "\n",
        "    # 2. Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            # Get source and target batches\n",
        "            src = batch['src'].to(device)\n",
        "            tgt = batch['tgt'].to(device)\n",
        "\n",
        "            # Create masks\n",
        "            src_mask, tgt_mask = create_masks(src, tgt)\n",
        "\n",
        "            # Prepare target for input and output\n",
        "            # Remove last token from target for input\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            # Remove first token from target for output\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "            # Reshape outputs and target for loss calculation\n",
        "            outputs = outputs.view(-1, outputs.size(-1))\n",
        "            tgt_output = tgt_output.view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, tgt_output)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Update loss tracking\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Print progress every N batches\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Calculate average loss for epoch\n",
        "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "        all_losses.append(avg_epoch_loss)\n",
        "        print(f\"Epoch {epoch + 1} Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_epoch_loss,\n",
        "        }, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "\n",
        "    return all_losses"
      ],
      "metadata": {
        "id": "xqHmQXVWcEFD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7wW1M1kesIL",
        "outputId": "a82e528b-8093-4f7c-9977-baedfcbd4b74"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting de-core-news-sm==3.7.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from de-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import spacy\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def download_multi30k():\n",
        "    \"\"\"Download Multi30k dataset if not present\"\"\"\n",
        "    # Create data directory\n",
        "    if not os.path.exists('data'):\n",
        "        os.makedirs('data')\n",
        "\n",
        "    # Download files if they don't exist\n",
        "    base_url = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
        "    files = {\n",
        "        \"train.de\": \"train.de.gz\",\n",
        "        \"train.en\": \"train.en.gz\",\n",
        "        \"val.de\": \"val.de.gz\",\n",
        "        \"val.en\": \"val.en.gz\",\n",
        "        \"test.de\": \"test_2016_flickr.de.gz\",\n",
        "        \"test.en\": \"test_2016_flickr.en.gz\"\n",
        "    }\n",
        "\n",
        "    for local_name, remote_name in files.items():\n",
        "        filepath = f'data/{local_name}'\n",
        "        if not os.path.exists(filepath):\n",
        "            url = base_url + remote_name\n",
        "            urllib.request.urlretrieve(url, filepath + '.gz')\n",
        "            os.system(f'gunzip -f {filepath}.gz')\n",
        "\n",
        "def load_data(filename):\n",
        "    \"\"\"Load data from file\"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "def create_dataset():\n",
        "    \"\"\"Create dataset from files\"\"\"\n",
        "    # Download data if needed\n",
        "    download_multi30k()\n",
        "\n",
        "    # Load data\n",
        "    train_de = load_data('data/train.de')\n",
        "    train_en = load_data('data/train.en')\n",
        "    val_de = load_data('data/val.de')\n",
        "    val_en = load_data('data/val.en')\n",
        "\n",
        "    return (train_de, train_en), (val_de, val_en)\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_texts, tgt_texts, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer):\n",
        "        self.src_texts = src_texts\n",
        "        self.tgt_texts = tgt_texts\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.src_texts[idx]\n",
        "        tgt_text = self.tgt_texts[idx]\n",
        "\n",
        "        # Tokenize\n",
        "        src_tokens = [tok.text for tok in self.src_tokenizer(src_text)]\n",
        "        tgt_tokens = [tok.text for tok in self.tgt_tokenizer(tgt_text)]\n",
        "\n",
        "        # Convert to indices\n",
        "        src_indices = [self.src_vocab[\"<s>\"]] + [self.src_vocab.get(token, self.src_vocab[\"<unk>\"]) for token in src_tokens] + [self.src_vocab[\"</s>\"]]\n",
        "        tgt_indices = [self.tgt_vocab[\"<s>\"]] + [self.tgt_vocab.get(token, self.tgt_vocab[\"<unk>\"]) for token in tgt_tokens] + [self.tgt_vocab[\"</s>\"]]\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src_indices),\n",
        "            'tgt': torch.tensor(tgt_indices)\n",
        "        }\n",
        "\n",
        "def build_vocab_from_texts(texts, tokenizer, min_freq=2):\n",
        "    \"\"\"Build vocabulary from texts\"\"\"\n",
        "    counter = {}\n",
        "    for text in texts:\n",
        "        for token in [tok.text for tok in tokenizer(text)]:\n",
        "            counter[token] = counter.get(token, 0) + 1\n",
        "\n",
        "    # Create vocabulary\n",
        "    vocab = {\"<s>\": 0, \"</s>\": 1, \"<blank>\": 2, \"<unk>\": 3}\n",
        "    idx = 4\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "    return vocab\n",
        "\n",
        "def create_dataloaders(batch_size=32):\n",
        "    # Load tokenizers\n",
        "    spacy_de = spacy.load(\"de_core_news_sm\")\n",
        "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Get data\n",
        "    (train_de, train_en), (val_de, val_en) = create_dataset()\n",
        "\n",
        "    # Build vocabularies\n",
        "    vocab_src = build_vocab_from_texts(train_de, spacy_de)\n",
        "    vocab_tgt = build_vocab_from_texts(train_en, spacy_en)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TranslationDataset(\n",
        "        train_de, train_en,\n",
        "        vocab_src, vocab_tgt,\n",
        "        spacy_de, spacy_en\n",
        "    )\n",
        "\n",
        "    val_dataset = TranslationDataset(\n",
        "        val_de, val_en,\n",
        "        vocab_src, vocab_tgt,\n",
        "        spacy_de, spacy_en\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_batch\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_batch\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader, vocab_src, vocab_tgt\n",
        "\n",
        "def collate_batch(batch):\n",
        "    src_tensors = [item['src'] for item in batch]\n",
        "    tgt_tensors = [item['tgt'] for item in batch]\n",
        "\n",
        "    # Pad sequences\n",
        "    src_padded = torch.nn.utils.rnn.pad_sequence(src_tensors, batch_first=True, padding_value=2)\n",
        "    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_tensors, batch_first=True, padding_value=2)\n",
        "\n",
        "    return {\n",
        "        'src': src_padded,\n",
        "        'tgt': tgt_padded\n",
        "    }\n",
        "#Usage\n",
        "train_dataloader, val_dataloader, vocab_src, vocab_tgt = create_dataloaders(batch_size=32)"
      ],
      "metadata": {
        "id": "yUp1NExScGvr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Assuming 'model' is your Transformer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.95)\n",
        "\n",
        "# Initialize your transformer with the vocabulary sizes\n",
        "model = Transformer(\n",
        "    src_vocab_size=len(vocab_src),\n",
        "    tgt_vocab_size=len(vocab_tgt),\n",
        "    d_model=512,\n",
        "    num_layers=6,\n",
        "    num_heads=8,\n",
        "    d_ff=2048,\n",
        "    dropout=0.1\n",
        ")\n",
        "criterion = LabelSmoothing(smoothing=0.1).to(device)\n",
        "\n",
        "# Now you can use your training loop\n",
        "losses = train_transformer(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "PfyuxzPYcU69",
        "outputId": "ee80c97f-da25-434f-d699-03f967849d7d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-e63d6617780a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Now you can use your training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m losses = train_transformer(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-3337774b3804>\u001b[0m in \u001b[0;36mtrain_transformer\u001b[0;34m(model, train_dataloader, criterion, optimizer, scheduler, num_epochs, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Create masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Prepare target for input and output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f4f33363a177>\u001b[0m in \u001b[0;36mcreate_masks\u001b[0;34m(src, tgt)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# 3. Combine padding and future mask for target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Both masks should be True for allowed positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_padding_mask\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtgt_future_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ]
    }
  ]
}