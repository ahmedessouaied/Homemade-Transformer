# Transformer Implementation: Attention Is All You Need

[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C.svg)](https://pytorch.org/)

An implementation of the Transformer architecture from the paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017) using PyTorch, designed for sequence-to-sequence tasks like machine translation.

---

## Features
- **Multi-Head Self-Attention**: Parallel attention mechanisms.
- **Positional Encoding**: Sinusoidal embeddings for sequence order.
- **Encoder-Decoder Architecture**: 6-layer stacks with residual connections.
- **Masking**: Padding masks for encoder, future masks for decoder.
- **Label Smoothing**: Improved training stability.

---
